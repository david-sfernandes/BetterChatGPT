{
  "configuration": "Configuração",
  "model": "Modelo",
  "token": {
    "label": "Máximo de Tokens",
    "description": "O número máximo de tokens a serem gerados na conclusão do chat. O comprimento total dos tokens de entrada e dos tokens gerados é limitado pelo comprimento do contexto do modelo."
  },
  "default": "Padrão",
  "temperature": {
    "label": "Temperatura",
    "description": "Qual temperatura de amostragem usar, entre 0 e 2. Valores mais altos como 0.8 tornarão a saída mais aleatória, enquanto valores mais baixos como 0.2 a tornarão mais focada e determinística. Geralmente recomendamos alterar isso ou top p, mas não ambos. (Padrão: 1)"
  },
  "presencePenalty": {
    "label": "Penalidade de Presença",
    "description": "Número entre -2.0 e 2.0. Valores positivos penalizam novos tokens com base em se eles aparecem no texto até agora, aumentando a probabilidade do modelo falar sobre novos tópicos. (Padrão: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Número entre 0 e 1. Uma alternativa à amostragem com temperatura, chamada amostragem de núcleo, onde o modelo considera os resultados dos tokens com o núcleo superior p de massa de probabilidade. Então, 0.1 significa que apenas os tokens que compõem os 10% superiores da massa de probabilidade são considerados. Geralmente recomendamos alterar isso ou a temperatura, mas não ambos. (Padrão: 1)"
  },
  "frequencyPenalty": {
    "label": "Penalidade de Frequência",
    "description": "Número entre -2.0 e 2.0. Valores positivos penalizam novos tokens com base em sua frequência existente no texto até agora, diminuindo a probabilidade do modelo repetir a mesma linha textualmente. (Padrão: 0)"
  },
  "defaultChatConfig": "Configuração de Chat Padrão",
  "defaultSystemMessage": "Mensagem de Sistema Padrão",
  "resetToDefault": "Redefinir para Padrão"
}